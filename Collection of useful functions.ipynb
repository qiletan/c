{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here are some useful functions I found from this competition \n",
    "\n",
    "https://www.kaggle.com/aantonova/collection-of-useful-functions-and-simple-baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import datetime, time\n",
    "import warnings\n",
    "warnings.simplefilter(action = 'ignore', category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split\n",
    "from sklearn.preprocessing import scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ranksums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Service functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df_):\n",
    "    start_mem = df_.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for c in df_.columns[df_.dtypes != 'object']:\n",
    "        col_type = df_[c].dtype\n",
    "        \n",
    "        c_min = df_[c].min()\n",
    "        c_max = df_[c].max()\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df_[c] = df_[c].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df_[c] = df_[c].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df_[c] = df_[c].astype(np.int32)\n",
    "            else:\n",
    "                df_[c] = df_[c].astype(np.int64)  \n",
    "        else:\n",
    "            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                df_[c] = df_[c].astype(np.float16)\n",
    "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df_[c] = df_[c].astype(np.float32)\n",
    "            else:\n",
    "                df_[c] = df_[c].astype(np.float64)\n",
    "\n",
    "    end_mem = df_.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For exploring missing values across train and test sets\n",
    "def get_missing_report(df_, target_name = 'TARGET'):\n",
    "    # Divide in training/validation and test data\n",
    "    df_train_ = df_[df_[target_name].notnull()].drop(target_name, axis = 1)\n",
    "    df_test_ = df_[df_[target_name].isnull()].drop(target_name, axis = 1)\n",
    "    \n",
    "    count_missing_train = df_train_.isnull().sum().values\n",
    "    ratio_missing_train = count_missing_train / df_train_.shape[0]\n",
    "    count_missing_test = df_test_.isnull().sum().values\n",
    "    ratio_missing_test = count_missing_test / df_test_.shape[0]\n",
    "    \n",
    "    return pd.DataFrame(data = {'count_missing_train': count_missing_train, \n",
    "                                'ratio_missing_train': ratio_missing_train,\n",
    "                                'count_missing_test': count_missing_test, \n",
    "                                'ratio_missing_test': ratio_missing_test}, \n",
    "                        index = df_test_.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparing distributions of features by target =1/0\n",
    "def corr_feature_with_target(feature, target):\n",
    "    c0 = feature[target == 0].dropna()\n",
    "    c1 = feature[target == 1].dropna()\n",
    "        \n",
    "    if set(feature.unique()) == set([0, 1]):\n",
    "        diff = abs(c0.mean(axis = 0) - c1.mean(axis = 0))\n",
    "    else:\n",
    "        diff = abs(c0.median(axis = 0) - c1.median(axis = 0))\n",
    "        \n",
    "    p = ranksums(c0, c1)[1] if ((len(c0) >= 20) & (len(c1) >= 20)) else 2\n",
    "        \n",
    "    return [diff / feature.mean(), p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving scores and metrics\n",
    "scores_index = [\n",
    "        'roc_auc_train', 'roc_auc_test', \n",
    "        'precision_train_0', 'precision_test_0', \n",
    "        'precision_train_1', 'precision_test_1', \n",
    "        'recall_train_0', 'recall_test_0', \n",
    "        'recall_train_1', 'recall_test_1', \n",
    "        'LB'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visual analysis of the metrics\n",
    "def display_scores(df_scores_):\n",
    "    _, axes = plt.subplots(3, 2, figsize = (25, 10))\n",
    "    df_scores_.T[[scores_index[0]]].plot(ax = axes[0, 0]); # roc-auc train\n",
    "    df_scores_.T[[scores_index[1], scores_index[10]]].plot(ax = axes[0, 1]); # roc-auc test & LB\n",
    "    df_scores_.T[[scores_index[2], scores_index[3]]].plot(ax = axes[1, 0]);  # precision class 0\n",
    "    df_scores_.T[[scores_index[4], scores_index[5]]].plot(ax = axes[1, 1]);  # precision class 1\n",
    "    df_scores_.T[[scores_index[6], scores_index[7]]].plot(ax = axes[2, 0]);  # recall class 0\n",
    "    df_scores_.T[[scores_index[8], scores_index[9]]].plot(ax = axes[2, 1]);  # recall class 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cleaning float LGBM parameters after Bayesian optimization\n",
    "def int_lgbm_params(params):\n",
    "    for p in params:\n",
    "        if p in ['num_leaves', 'max_depth', 'n_estimators', 'subsample_for_bin', 'min_child_samples', \n",
    "                 'subsample_freq', 'random_state']:\n",
    "            params[p] = int(np.round(params[p], decimals = 0))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For cross-validation with LGBM classifier\n",
    "def cv_lgbm_scores(df_, num_folds, params, \n",
    "                   target_name = 'TARGET', index_name = 'SK_ID_CURR',\n",
    "                   stratified = False, rs = 1001, verbose = -1):\n",
    "    \n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    # Cleaning and defining parameters for LGBM\n",
    "    params = int_lgbm_params(params)\n",
    "    clf = LGBMClassifier(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\n",
    "\n",
    "    # Divide in training/validation and test data\n",
    "    df_train_ = df_[df_[target_name].notnull()]\n",
    "    df_test_ = df_[df_[target_name].isnull()]\n",
    "    print(\"Starting LightGBM cross-validation at {}\".format(time.ctime()))\n",
    "    print(\"Train shape: {}, test shape: {}\".format(df_train_.shape, df_test_.shape))\n",
    "\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits = num_folds, shuffle = True, random_state = rs)\n",
    "    else:\n",
    "        folds = KFold(n_splits = num_folds, shuffle = True, random_state = rs)\n",
    "        \n",
    "    # Create arrays to store results\n",
    "    train_pred = np.zeros(df_train_.shape[0])\n",
    "    train_pred_proba = np.zeros(df_train_.shape[0])\n",
    "\n",
    "    test_pred = np.zeros(df_train_.shape[0])\n",
    "    test_pred_proba = np.zeros(df_train_.shape[0])\n",
    "    \n",
    "    prediction = np.zeros(df_test_.shape[0]) # prediction for test set\n",
    "    \n",
    "    feats = df_train_.columns.drop([target_name, index_name])\n",
    "    \n",
    "    df_feat_imp_ = pd.DataFrame(index = feats)\n",
    "    \n",
    "    # Cross-validation cycle\n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_train_[feats], df_train_[target_name])):\n",
    "        print('--- Fold {} started at {}'.format(n_fold, time.ctime()))\n",
    "        \n",
    "        train_x, train_y = df_train_[feats].iloc[train_idx], df_train_[target_name].iloc[train_idx]\n",
    "        valid_x, valid_y = df_train_[feats].iloc[valid_idx], df_train_[target_name].iloc[valid_idx]\n",
    "\n",
    "        clf.fit(train_x, train_y, \n",
    "                eval_set = [(valid_x, valid_y)], eval_metric = 'auc', \n",
    "                verbose = verbose, early_stopping_rounds = 100)\n",
    "\n",
    "        train_pred[train_idx] = clf.predict(train_x, num_iteration = clf.best_iteration_)\n",
    "        train_pred_proba[train_idx] = clf.predict_proba(train_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        test_pred[valid_idx] = clf.predict(valid_x, num_iteration = clf.best_iteration_)\n",
    "        test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "        prediction += clf.predict_proba(df_test_[feats], \n",
    "                                        num_iteration = clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        df_feat_imp_[n_fold] = pd.Series(clf.feature_importances_, index = feats)\n",
    "        \n",
    "        del train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    # Computation of metrics\n",
    "    roc_auc_train = roc_auc_score(df_train_[target_name], train_pred_proba)\n",
    "    precision_train = precision_score(df_train_[target_name], train_pred, average = None)\n",
    "    recall_train = recall_score(df_train_[target_name], train_pred, average = None)\n",
    "    \n",
    "    roc_auc_test = roc_auc_score(df_train_[target_name], test_pred_proba)\n",
    "    precision_test = precision_score(df_train_[target_name], test_pred, average = None)\n",
    "    recall_test = recall_score(df_train_[target_name], test_pred, average = None)\n",
    "\n",
    "    print('Full AUC score {:.6f}'.format(roc_auc_test))\n",
    "    \n",
    "    # Filling the feature_importance table\n",
    "    df_feat_imp_.fillna(0, inplace = True)\n",
    "    df_feat_imp_['mean'] = df_feat_imp_.mean(axis = 1)\n",
    "    \n",
    "    # Preparing results of prediction for saving\n",
    "    prediction_train = df_train_[[index_name]]\n",
    "    prediction_train[target_name] = test_pred_proba\n",
    "    prediction_test = df_test_[[index_name]]\n",
    "    prediction_test[target_name] = prediction\n",
    "    \n",
    "    del df_train_, df_test_\n",
    "    gc.collect()\n",
    "    \n",
    "    # Returning the results and metrics in format for scores' table\n",
    "    return df_feat_imp_, prediction_train, prediction_test, \\\n",
    "           [roc_auc_train, roc_auc_test,\n",
    "            precision_train[0], precision_test[0], precision_train[1], precision_test[1],\n",
    "            recall_train[0], recall_test[0], recall_train[1], recall_test[1], 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visual analysis of the fearure importances\n",
    "def display_feature_importances(df_feat_imp_):\n",
    "    n_columns = 3\n",
    "    n_rows = (df_feat_imp_.shape[1] + 1) // n_columns\n",
    "    _, axes = plt.subplots(n_rows, n_columns, figsize=(8 * n_columns, 8 * n_rows))\n",
    "    for i, c in enumerate(df_feat_imp_.columns):\n",
    "        sns.barplot(x = c, y = 'index', \n",
    "                    data = df_feat_imp_.reset_index().sort_values(c, ascending = False).head(20), \n",
    "                    ax = axes[i // n_columns, i % n_columns] if n_rows > 1 else axes[i % n_columns])\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For selection of parameters for LGBM with Bayesian optimization\n",
    "def get_best_params_for_lgbm(df_train_, seed_cv_, seed_bo_, target_name = 'TARGET', \n",
    "                             init_points = 5, n_iter = 5):\n",
    "    def lgbm_auc_evaluate(**params):\n",
    "        warnings.simplefilter('ignore')\n",
    "    \n",
    "        params = int_lgbm_params(params)   \n",
    "        clf = LGBMClassifier(**params, n_estimators = 10000, nthread = 4, n_jobs = -1)\n",
    "\n",
    "        folds = KFold(n_splits = 2, shuffle = True, random_state = params['random_state'])\n",
    "        \n",
    "        test_pred_proba = np.zeros(df_train_.shape[0])\n",
    "    \n",
    "        feats = df_train_.columns.drop(target_name)\n",
    "    \n",
    "        for n_fold, (train_idx, valid_idx) in enumerate(folds.split(df_train_[feats], df_train_[target_name])):\n",
    "            train_x, train_y = df_train_[feats].iloc[train_idx], df_train_[target_name].iloc[train_idx]\n",
    "            valid_x, valid_y = df_train_[feats].iloc[valid_idx], df_train_[target_name].iloc[valid_idx]\n",
    "\n",
    "            clf.fit(train_x, train_y, \n",
    "                    eval_set = [(valid_x, valid_y)], eval_metric = 'auc', \n",
    "                    verbose = False, early_stopping_rounds = 100)\n",
    "\n",
    "            test_pred_proba[valid_idx] = clf.predict_proba(valid_x, num_iteration = clf.best_iteration_)[:, 1]\n",
    "        \n",
    "            del train_x, train_y, valid_x, valid_y\n",
    "            gc.collect()\n",
    "        \n",
    "        roc_auc_test = roc_auc_score(df_train_[target_name], test_pred_proba)\n",
    "        \n",
    "        return roc_auc_test\n",
    "    \n",
    "    params = {'learning_rate': (.001, .02), \n",
    "          'colsample_bytree': (0.3, 1),\n",
    "          'subsample_for_bin' : (20000, 500000),\n",
    "          'subsample': (0.3, 1), \n",
    "          'num_leaves': (2, 100), \n",
    "          'max_depth': (3, 9), \n",
    "          'reg_alpha': (.0, 1.), \n",
    "          'reg_lambda': (.0, 1.), \n",
    "          'min_split_gain': (.01, 1.),\n",
    "          'min_child_weight': (1, 50),\n",
    "          'min_child_samples': (10, 1000),\n",
    "          'random_state': (seed_cv_, seed_cv_)}\n",
    "    bo = BayesianOptimization(lgbm_auc_evaluate, params, random_state = seed_bo_)\n",
    "    bo.maximize(init_points = init_points, n_iter = n_iter)\n",
    "\n",
    "    return bo.res['max']['max_val'], bo.res['max']['max_params']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For blending predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For metadata about predictions\n",
    "blending_index = ['date', 'to_blend', 'folder', 'file_name', 'auc_train', 'auc_test', 'auc_LB', 'Comments']\n",
    "suffix_train = '_train.csv'\n",
    "suffix_test = '_test.csv'\n",
    "blending_folder = '../input/tmp-preds'\n",
    "blending_file_name = 'predictions_for_blending.csv'\n",
    "\n",
    "df_blend = pd.DataFrame(index = blending_index)\n",
    "df_blend.index.name = 'index'\n",
    "#df_blend.to_csv(blending_folder + '/' + blending_file_name) # Commented for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving prediction into file\n",
    "def save_prediction(df_, file_name, index_col = 'SK_ID_CURR', prediction_col = 'TARGET'):\n",
    "    df_.columns = [index_col, prediction_col]\n",
    "    df_.to_csv(file_name, index = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving files and metadata about predictions for blending\n",
    "def store_predictions_for_blending(df_train_, df_test_, file_name, scor, comments, \n",
    "                                   index_col = 'SK_ID_CURR', prediction_col = 'TARGET', \n",
    "                                   folder = blending_folder, b_file_name = blending_file_name):\n",
    "    \n",
    "    full_file_name = folder + '/' + file_name\n",
    "    save_prediction(df_train_, full_file_name + suffix_train, index_col, prediction_col)\n",
    "    save_prediction(df_test_, full_file_name + suffix_test, index_col, prediction_col)\n",
    "    \n",
    "    df_blending = pd.read_csv(b_file_name, index_col = 'index')\n",
    "    df_blending[df_blending.shape[1]] = [\n",
    "        datetime.datetime.today(),\n",
    "        True,\n",
    "        folder, file_name,\n",
    "        scor[0], scor[1], scor[-1],\n",
    "        comments\n",
    "    ]\n",
    "    df_blending.to_csv(b_file_name)\n",
    "    del df_blending\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For loading previous prediction results\n",
    "def load_predictions_for_blending(b_folder = blending_folder, b_file_name = blending_file_name):\n",
    "    def load_prediction(file_name):\n",
    "        tmp = pd.read_csv(file_name)\n",
    "        return tmp.set_index(tmp.columns[0])\n",
    "\n",
    "    df_blend_ = pd.read_csv(b_folder + '/' + b_file_name, index_col = 'index')\n",
    "    df_train_ = []\n",
    "    df_test_ = []\n",
    "    for c in df_blend_.columns:\n",
    "        #full_file_name = df_blend_.loc['folder', c] + '/' + df_blend_.loc['file_name', c]\n",
    "        full_file_name = '../input/tmp-preds' + '/' + df_blend_.loc['file_name', c] # Only for Kaggle\n",
    "        df_train_.append(load_prediction(full_file_name + suffix_train))\n",
    "        df_test_.append(load_prediction(full_file_name + suffix_test))\n",
    "        \n",
    "    df_train_ = pd.concat(df_train_, axis = 1)\n",
    "    df_train_.columns = df_blend_.columns\n",
    "    df_test_ = pd.concat(df_test_, axis = 1)\n",
    "    df_test_.columns = df_blend_.columns\n",
    "\n",
    "    return df_train_, df_test_, df_blend_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For blending flagged predictions\n",
    "def get_blended_prediction(df_train_, flag, params_):\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    test_pred_proba = pd.Series(np.zeros(df_train_.shape[0]), index = df_train_.index)\n",
    "    \n",
    "    for f in df_train_.columns[flag.values.astype(bool)]:\n",
    "        test_pred_proba += df_train_[f] * params_[f]\n",
    "        \n",
    "    min_pr = test_pred_proba.min()\n",
    "    max_pr = test_pred_proba.max()\n",
    "    test_pred_proba = (test_pred_proba - min_pr) / (max_pr - min_pr)\n",
    "    return test_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For selection of parameters with Bayesian optimization\n",
    "def get_best_params_for_blending(df_train_, flag, target, seed_bo_, init_points = 10, n_iter = 10):\n",
    "    def blend_auc_evaluate(**params):\n",
    "        return roc_auc_score(target, get_blended_prediction(df_train_, flag, params))    \n",
    "    \n",
    "    params = {}\n",
    "    for c in df_train_.columns[flag.values.astype(bool)]:\n",
    "        params[c] = (0, 1)\n",
    "\n",
    "    bo = BayesianOptimization(blend_auc_evaluate, params, random_state = seed_bo_)\n",
    "    bo.maximize(init_points = init_points, n_iter = n_iter)\n",
    "\n",
    "    return bo.res['max']['max_val'], bo.res['max']['max_params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
